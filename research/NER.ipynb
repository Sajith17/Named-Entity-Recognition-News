{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ttn6tUvB6rWv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.config.list_physical_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.10.0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3Nwyqvkp8Em",
        "outputId": "ad9dbd12-11c5-4ab9-a24e-cb0094b881cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sajit\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, load_from_disk\n",
        "data = load_dataset('conll2003')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJdsCF4-GL4e",
        "outputId": "3a5f5950-a5c6-4fc0-e406-4d3c5ffa1f56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['train'].features['ner_tags'].feature.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gx84KbdOCAT",
        "outputId": "9ed0163d-37cd-4808-d1ca-8da654f4b790"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17682"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oov_token = '[UNK]'\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token, lower=True)\n",
        "tokenizer.fit_on_texts([' '.join(x) for x in data['train']['tokens']])\n",
        "len(tokenizer.word_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 30, 2, 517]]"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([\"Russia's is the best\".split(' ')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pMBIPQa5DlZB"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
        "  tokenized_inputs = [tokenizer.texts_to_sequences(token) for token in examples['tokens']]\n",
        "  new_tokenized_inputs = []\n",
        "  labels = []\n",
        "  word_ids_list = []\n",
        "  for i,tokenized_input in enumerate(tokenized_inputs):\n",
        "    ner_tags = examples['ner_tags'][i]\n",
        "    label_ids = []\n",
        "    word_ids = []\n",
        "    tokenized_sentence = []\n",
        "    for j,tokenized_words in enumerate(tokenized_input):\n",
        "      if tokenized_words:\n",
        "        tokenized_sentence.extend(tokenized_words)\n",
        "        word_ids.extend([j]*len(tokenized_words))\n",
        "        label_ids.append(ner_tags[j])\n",
        "        for k in range(len(tokenized_words)-1):\n",
        "          label_ids.append(ner_tags[j] if label_all_tokens else -1)\n",
        "    labels.append(label_ids)\n",
        "    word_ids_list.append(word_ids)\n",
        "    new_tokenized_inputs.append(tokenized_sentence)\n",
        "  return {'input_ids': new_tokenized_inputs, 'word_ids': word_ids_list, 'labels': labels}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iikUQXd3793B"
      },
      "outputs": [],
      "source": [
        "def input_and_label_pad_sequence(examples, maxlen=100):\n",
        "  return { 'input_ids': tf.keras.preprocessing.sequence.pad_sequences(examples['input_ids'],maxlen = maxlen, padding='post', truncating='post'),\n",
        "           'labels': tf.keras.preprocessing.sequence.pad_sequences(examples['labels'],maxlen = maxlen, padding='post', truncating='post',value=-1 )}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_padding_mask(examples):\n",
        "    seq = 1 - (np.array(examples['input_ids'])==0)\n",
        "    return {\"attention_mask\": seq}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NDjTmgH_LH3D"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        }
      ],
      "source": [
        "maxlen = 100\n",
        "\n",
        "data = data.map(tokenize_and_align_labels, batched=True).map(input_and_label_pad_sequence, batched=True).map(create_padding_mask, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'word_ids', 'labels', 'attention_mask'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'word_ids', 'labels', 'attention_mask'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'word_ids', 'labels', 'attention_mask'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = tf.data.Dataset.from_tensor_slices((data['train']['input_ids'],data['train']['attention_mask'],data['train']['labels'])).map(lambda x,y,z: (tf.concat((x,y),axis=-1),z)).shuffle(10000).batch(64)\n",
        "validation = tf.data.Dataset.from_tensor_slices((data['validation']['input_ids'],data['validation']['attention_mask'],data['validation']['labels'])).map(lambda x,y,z: (tf.concat((x,y),axis=-1),z)).batch(64)\n",
        "test = tf.data.Dataset.from_tensor_slices((data['test']['input_ids'],data['test']['attention_mask'])).map(lambda x,y: tf.concat((x,y),axis=-1)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54/54 [==============================] - 1s 10ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[[8.95207465e-01, 1.82132777e-02, 5.95899764e-03, ...,\n",
              "         2.62658554e-03, 1.26864361e-02, 3.54189542e-03],\n",
              "        [3.62532400e-02, 2.01158792e-01, 1.39169052e-01, ...,\n",
              "         3.42726000e-02, 3.18115242e-02, 1.52590610e-02],\n",
              "        [8.64837825e-01, 4.59051467e-02, 1.70826111e-02, ...,\n",
              "         4.34076972e-03, 9.14051570e-03, 6.25726022e-03],\n",
              "        ...,\n",
              "        [9.43994641e-01, 1.05073247e-02, 1.17700137e-02, ...,\n",
              "         2.51220190e-03, 2.92870705e-03, 6.11473108e-03],\n",
              "        [9.30240035e-01, 1.40919713e-02, 1.41257774e-02, ...,\n",
              "         2.88459356e-03, 3.63089913e-03, 8.22029542e-03],\n",
              "        [9.00064468e-01, 2.46056076e-02, 2.10508071e-02, ...,\n",
              "         3.69378296e-03, 5.13632968e-03, 1.21017955e-02]],\n",
              "\n",
              "       [[6.47409707e-02, 1.32310107e-01, 6.47920966e-02, ...,\n",
              "         4.96314056e-02, 3.96114588e-02, 3.51920649e-02],\n",
              "        [6.40925243e-02, 1.87245995e-01, 8.35430771e-02, ...,\n",
              "         4.44083698e-02, 4.11222540e-02, 4.45025414e-02],\n",
              "        [9.89445001e-02, 2.33645469e-01, 1.31098032e-01, ...,\n",
              "         7.14380890e-02, 3.44440006e-02, 4.18189168e-02],\n",
              "        ...,\n",
              "        [5.40584624e-01, 3.95011902e-02, 6.97868615e-02, ...,\n",
              "         5.75036220e-02, 1.37592601e-02, 2.23297402e-02],\n",
              "        [5.22015989e-01, 5.34276962e-02, 8.29078332e-02, ...,\n",
              "         5.50717488e-02, 1.37085775e-02, 2.91259885e-02],\n",
              "        [4.32764977e-01, 9.05643329e-02, 1.06978953e-01, ...,\n",
              "         5.86883612e-02, 1.90596357e-02, 3.93195078e-02]],\n",
              "\n",
              "       [[5.23696765e-02, 9.64992940e-02, 5.05216420e-02, ...,\n",
              "         3.41808051e-02, 2.97709536e-02, 1.31667387e-02],\n",
              "        [9.77726206e-02, 2.31178984e-01, 1.01141721e-01, ...,\n",
              "         3.90789583e-02, 4.05894220e-02, 3.80760953e-02],\n",
              "        [2.88042873e-02, 1.36186481e-01, 1.31692752e-01, ...,\n",
              "         4.14007120e-02, 4.83923107e-02, 4.55955677e-02],\n",
              "        ...,\n",
              "        [8.93217564e-01, 1.79361925e-02, 2.24844199e-02, ...,\n",
              "         6.53754920e-03, 4.46275622e-03, 9.07677785e-03],\n",
              "        [8.76725078e-01, 2.25359052e-02, 2.51959711e-02, ...,\n",
              "         7.03325123e-03, 5.24704019e-03, 1.19131282e-02],\n",
              "        [8.19847047e-01, 4.19388823e-02, 3.84855568e-02, ...,\n",
              "         9.11224447e-03, 7.64001487e-03, 1.81533769e-02]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[5.21124661e-01, 1.06426060e-01, 3.71051989e-02, ...,\n",
              "         1.86462738e-02, 5.44512384e-02, 1.68939643e-02],\n",
              "        [7.36222148e-01, 5.92942536e-02, 2.28670835e-02, ...,\n",
              "         3.72863724e-03, 1.86266992e-02, 7.45566376e-03],\n",
              "        [2.00506523e-01, 2.00802892e-01, 1.35319695e-01, ...,\n",
              "         1.10846860e-02, 4.15616035e-02, 2.07372978e-02],\n",
              "        ...,\n",
              "        [9.74517226e-01, 4.23796661e-03, 4.41734912e-03, ...,\n",
              "         9.41479870e-04, 2.06818548e-03, 3.90661322e-03],\n",
              "        [9.68613029e-01, 5.58824930e-03, 5.52771846e-03, ...,\n",
              "         1.08540407e-03, 2.46114167e-03, 5.25494386e-03],\n",
              "        [9.59759414e-01, 8.59952718e-03, 7.26567721e-03, ...,\n",
              "         1.23780477e-03, 3.15216929e-03, 7.08565488e-03]],\n",
              "\n",
              "       [[9.63883400e-01, 1.00780679e-02, 1.72819465e-03, ...,\n",
              "         3.06982402e-04, 4.05664323e-03, 9.88331391e-04],\n",
              "        [3.73941272e-01, 2.83076704e-01, 6.35701939e-02, ...,\n",
              "         6.09007524e-03, 1.49771878e-02, 9.28131118e-03],\n",
              "        [1.42112583e-01, 2.04404220e-01, 1.51125997e-01, ...,\n",
              "         1.42521430e-02, 4.12440971e-02, 2.40986906e-02],\n",
              "        ...,\n",
              "        [9.55941141e-01, 7.22108409e-03, 7.87569862e-03, ...,\n",
              "         1.85881031e-03, 3.31722572e-03, 6.59351842e-03],\n",
              "        [9.48671401e-01, 8.95208865e-03, 9.38916486e-03, ...,\n",
              "         2.07185745e-03, 3.69900186e-03, 8.38045403e-03],\n",
              "        [9.32418346e-01, 1.45139461e-02, 1.26740998e-02, ...,\n",
              "         2.39450554e-03, 5.04091289e-03, 1.13305198e-02]],\n",
              "\n",
              "       [[9.87412095e-01, 2.06106016e-03, 3.22473730e-04, ...,\n",
              "         2.20657108e-04, 1.59161945e-03, 6.01792941e-04],\n",
              "        [1.70643657e-01, 1.04570657e-01, 7.52983764e-02, ...,\n",
              "         1.59269143e-02, 5.77085018e-02, 1.58682670e-02],\n",
              "        [9.58756447e-01, 1.25164716e-02, 3.29104997e-03, ...,\n",
              "         5.35532192e-04, 4.27304022e-03, 1.17114128e-03],\n",
              "        ...,\n",
              "        [9.58108962e-01, 6.79945061e-03, 7.27726705e-03, ...,\n",
              "         1.72119727e-03, 3.27160209e-03, 6.37295330e-03],\n",
              "        [9.50758517e-01, 8.58918671e-03, 8.75641312e-03, ...,\n",
              "         1.92498742e-03, 3.67271388e-03, 8.19482375e-03],\n",
              "        [9.35157120e-01, 1.38746118e-02, 1.18620526e-02, ...,\n",
              "         2.25390913e-03, 5.01100859e-03, 1.10848546e-02]]], dtype=float32)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer.predict(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eOy8izYqdpGD"
      },
      "outputs": [],
      "source": [
        "# @tf.keras.utils.register_keras_serializable()\n",
        "def positional_encoding(positions, d_model):\n",
        "\n",
        "    position = np.arange(positions)[:, np.newaxis]\n",
        "    k = np.arange(d_model)[np.newaxis, :]\n",
        "    i = k // 2\n",
        "\n",
        "    # initialize a matrix angle_rads of all the angles\n",
        "    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n",
        "    angle_rads = position * angle_rates\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "07gEyi_3dt7N"
      },
      "outputs": [],
      "source": [
        "# @tf.keras.utils.register_keras_serializable()\n",
        "def create_padding_mask(decoder_token_ids):\n",
        "\n",
        "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "90S6GyqBd90_"
      },
      "outputs": [],
      "source": [
        "# @tf.keras.utils.register_keras_serializable()\n",
        "def FullyConnected(embedding_dim, fully_connected_dim):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(fully_connected_dim, activation='relu'),\n",
        "      tf.keras.layers.Dense(embedding_dim)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lNTAEMtKes7K"
      },
      "outputs": [],
      "source": [
        "# @tf.keras.utils.register_keras_serializable()\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
        "               dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=embedding_dim,\n",
        "        dropout=dropout_rate\n",
        "    )\n",
        "\n",
        "    self.ffn = FullyConnected(\n",
        "        embedding_dim=embedding_dim,\n",
        "        fully_connected_dim=fully_connected_dim\n",
        "    )\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "    self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    self_mha_output = self.mha(x,x,x,mask)\n",
        "    skip_x_attention = self.layernorm1(x+self_mha_output)\n",
        "    ffn_output = self.ffn(skip_x_attention)\n",
        "    ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "    encoder_layer_out = self.layernorm2(skip_x_attention+ffn_output)\n",
        "\n",
        "    return encoder_layer_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U4wLAtb2gpzq"
      },
      "outputs": [],
      "source": [
        "# @tf.keras.utils.register_keras_serializable()\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, vocab_size,\n",
        "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            self.embedding_dim)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(embedding_dim=embedding_dim,\n",
        "                                    num_heads=num_heads,\n",
        "                                    fully_connected_dim=fully_connected_dim,\n",
        "                                    dropout_rate=dropout_rate,\n",
        "                                    layernorm_eps=layernorm_eps\n",
        "                                    )\n",
        "                      for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "     seq_len = tf.shape(x)[1]\n",
        "\n",
        "     x = self.embedding(x)\n",
        "\n",
        "     x*=tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "\n",
        "     x+=self.pos_encoding[:,:seq_len,:]\n",
        "\n",
        "     x = self.dropout(x, training=training)\n",
        "\n",
        "     for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "     return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rpr8oO_-kdGb"
      },
      "outputs": [],
      "source": [
        "# @tf.keras.utils.register_keras_serializable()\n",
        "class NERModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_tags, num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
        "               vocab_size, max_positional_encoding,dropout_rate=0.1,\n",
        "               layernorm_eps=1e-6):\n",
        "\n",
        "    super(NERModel, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(     num_layers=num_layers,\n",
        "                                embedding_dim=embedding_dim,\n",
        "                                num_heads=num_heads,\n",
        "                                fully_connected_dim=fully_connected_dim,\n",
        "                                vocab_size=vocab_size,\n",
        "                                maximum_position_encoding=max_positional_encoding,\n",
        "                                dropout_rate=dropout_rate,\n",
        "                                layernorm_eps=layernorm_eps)\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "    self.ffn = tf.keras.layers.Dense(fully_connected_dim, activation='relu')\n",
        "    self.ffn_final = tf.keras.layers.Dense(num_tags,activation='softmax')\n",
        "\n",
        "  def call(self, x, training):\n",
        "    seq_length = tf.shape(x)[-1]//2\n",
        "    mask = x[:,seq_length:]\n",
        "    mask = mask[:,tf.newaxis,:]\n",
        "    x = x[:,:seq_length]\n",
        "    x = self.encoder(x,training, mask)\n",
        "    x = self.dropout1(x, training = training)\n",
        "    x = self.ffn(x)\n",
        "    x = self.dropout2(x, training=training)\n",
        "    x = self.ffn_final(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHGwxs2SBOhi",
        "outputId": "e9b2404a-8010-4650-ab2f-96cb75ea57c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['train'].features['ner_tags']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17680"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_layers = 2\n",
        "num_tags = 9\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "embedding_dim = 100\n",
        "fully_connected_dim = 100\n",
        "num_heads = 3\n",
        "positional_encoding_length = 150\n",
        "\n",
        "# transformer = NERModel(\n",
        "#     num_tags,\n",
        "#     num_layers,\n",
        "#     embedding_dim,\n",
        "#     num_heads,\n",
        "#     fully_connected_dim,\n",
        "#     vocab_size,\n",
        "#     positional_encoding_length,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DWaiNh7sJQ2v"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, name=\"custom_ner_loss\",**kwargs):\n",
        "        super().__init__(name=name,**kwargs)\n",
        "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        mask = tf.math.logical_not(tf.math.equal(y_true, -1))\n",
        "        mask = tf.cast(mask, dtype=tf.int32)\n",
        "\n",
        "        loss = self.loss_object(y_true*mask, y_pred)\n",
        "\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "\n",
        "        return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8WsUjdM7zxk2"
      },
      "outputs": [],
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(y_true, -1))\n",
        "    mask = tf.cast(mask,dtype=tf.float32)\n",
        "    pred = tf.cast(tf.argmax(y_pred,axis=-1),dtype=tf.float32)\n",
        "    correct = tf.cast(tf.equal(y_true,pred),dtype=tf.float32)\n",
        "    correct*=mask\n",
        "    return tf.reduce_sum(correct)/tf.reduce_sum(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Cum0pT6pDfvx"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(embedding_dim)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "PfV439uZLU_L"
      },
      "outputs": [],
      "source": [
        "transformer.compile(optimizer=optimizer, loss=[MaskedLoss()], metrics=[masked_acc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "220/220 [==============================] - 8s 29ms/step - loss: 1.4129 - acc: 0.5930 - val_loss: 0.8599 - val_acc: 0.7898\n",
            "Epoch 2/10\n",
            "169/220 [======================>.......] - ETA: 1s - loss: 0.7510 - acc: 0.8174"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4776\\3800935456.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m transformer.fit(train,\n\u001b[0;32m      2\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                 epochs = 10)\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sajit\\anaconda3\\envs\\tf(GPU)3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sajit\\anaconda3\\envs\\tf(GPU)3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sajit\\anaconda3\\envs\\tf(GPU)3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sajit\\anaconda3\\envs\\tf(GPU)3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2496\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2497\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2499\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sajit\\anaconda3\\envs\\tf(GPU)3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1863\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sajit\\anaconda3\\envs\\tf(GPU)3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[1;32mc:\\Users\\sajit\\anaconda3\\envs\\tf(GPU)3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "transformer.fit(train,\n",
        "                validation_data=(validation),\n",
        "                epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Exception encountered when calling layer \"ner_model_4\" \"                 f\"(type NERModel).\n\nAttempt to convert a value (<BatchDataset element_spec=TensorSpec(shape=(None, 200), dtype=tf.int32, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>) to a Tensor.\n\nCall arguments received by layer \"ner_model_4\" \"                 f\"(type NERModel):\n  • x=<BatchDataset element_spec=TensorSpec(shape=(None, 200), dtype=tf.int32, name=None)>\n  • training=None",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4776\\105310786.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4776\\2207286106.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, training)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mseq_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"ner_model_4\" \"                 f\"(type NERModel).\n\nAttempt to convert a value (<BatchDataset element_spec=TensorSpec(shape=(None, 200), dtype=tf.int32, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>) to a Tensor.\n\nCall arguments received by layer \"ner_model_4\" \"                 f\"(type NERModel):\n  • x=<BatchDataset element_spec=TensorSpec(shape=(None, 200), dtype=tf.int32, name=None)>\n  • training=None"
          ]
        }
      ],
      "source": [
        "transformer(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmRyRe3rQ8l1",
        "outputId": "886225f0-8859-4103-b342-bd5ba9705dbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['RUGBY',\n",
              "  'UNION',\n",
              "  '-',\n",
              "  'CUTTITTA',\n",
              "  'BACK',\n",
              "  'FOR',\n",
              "  'ITALY',\n",
              "  'AFTER',\n",
              "  'A',\n",
              "  'YEAR',\n",
              "  '.'],\n",
              " [3, 4, 0, 1, 0, 0, 5, 0, 0, 0, 0],\n",
              " [0, 1, 3, 4, 5, 6, 7, 8, 9],\n",
              " [3, 4, 1, 0, 0, 5, 0, 0, 0])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = data['test'][22]\n",
        "example['tokens'],example['ner_tags'],example['word_ids'],example['labels'][:len(example['word_ids'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40faUOENRbAa",
        "outputId": "7953cdce-89a1-43d0-8e57-230fd7d9b3fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54/54 [==============================] - 1s 14ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[3,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 3,\n",
              " 3,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 3,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 7,\n",
              " 7,\n",
              " 7,\n",
              " 7,\n",
              " 3,\n",
              " 7,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[np.argmax(x) for x in transformer.predict(test)[22]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_list = data['train'].features['ner_tags'].feature.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score \n",
        "\n",
        "def compute_metrics(labels, pred_logits):\n",
        "\n",
        "    pred_logits = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    predictions = []\n",
        "    for prediction, label in zip(pred_logits, labels):\n",
        "        for eval_pred, l in zip(prediction, label):\n",
        "            if l!= -1:\n",
        "                predictions.append(eval_pred)\n",
        "\n",
        "    true_labels = []\n",
        "    for prediction, label in zip(pred_logits, labels):\n",
        "        for eval_pred, l in zip(prediction, label):\n",
        "            if l!= -1:\n",
        "                true_labels.append(l)\n",
        "\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    \n",
        "    return {'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1 }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54/54 [==============================] - 1s 14ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'precision': 0.7354912726458616,\n",
              " 'recall': 0.6560956424169582,\n",
              " 'f1_score': 0.6889829013166076}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = data['test']['labels']\n",
        "pred_logits = transformer.predict(test)\n",
        "\n",
        "compute_metrics((pred_logits,labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('artifacts\\model_trainer\\model.keras', custom_objects={\n",
        "            'NERModel':NERModel,\n",
        "            'MaskedLoss':MaskedLoss,\n",
        "            'masked_acc': masked_acc,\n",
        "            'CustomSchedule': CustomSchedule\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "from named_entity_recognition.config.configuration import ConfigurationManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PredictionPipeline:\n",
        "    def __init__(self):\n",
        "        manager = ConfigurationManager()\n",
        "        self.config = manager.get_data_transformation_config()\n",
        "        self.model_path = manager.get_model_evaluation_config().model_path\n",
        "        self.tags = {1:\"Person\", 3:\"Organization\", 5: \"Location\", 7: \"Miscellaneous\"}\n",
        "    def predict(self, sentence):\n",
        "        model = self.get_model()\n",
        "        input_ids, word_ids = self.get_input_ids_and_word_ids(sentence)\n",
        "        input_ids = self.pad_and_mask([input_ids])\n",
        "        labels = tf.argmax(model.predict(input_ids),axis=-1).numpy().tolist()[0]\n",
        "        labels = self.align_labels(word_ids,labels)\n",
        "        return self.get_entities(sentence,labels)\n",
        "\n",
        "    def get_entities(self, sentence, labels):\n",
        "        sentence = sentence.split(' ')\n",
        "        entities = []\n",
        "        current_span = ''\n",
        "        prev_label = 0\n",
        "        for word, label in zip(sentence, labels):\n",
        "            if label == 0 and current_span:\n",
        "                entities.append([current_span, self.tags[prev_label]])\n",
        "                current_span = ''\n",
        "                prev_label = 0\n",
        "            elif label in {1,3,5,7}:\n",
        "                if current_span:\n",
        "                    entities.append([current_span, self.tags[prev_label]])\n",
        "                current_span = word\n",
        "                prev_label = label\n",
        "            elif prev_label and label in {2,4,6,8}:\n",
        "                current_span += ' ' + word\n",
        "        if current_span:\n",
        "            entities.append([current_span, self.tags[prev_label]])\n",
        "        return entities\n",
        "\n",
        "    def align_labels(self, word_ids, labels):\n",
        "        aligned_labels = [0]*(max(word_ids)+1)\n",
        "        for word_id, label in zip(word_ids, labels):\n",
        "            if aligned_labels[word_id] == 0 and label:\n",
        "                aligned_labels[word_id] = label\n",
        "        return aligned_labels\n",
        "    \n",
        "    def pad_and_mask(self, input_ids):\n",
        "        input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=self.config.params_max_sequence_length, padding='post', truncating='post')\n",
        "        mask = tf.cast(tf.math.not_equal(input_ids, 0), tf.int32)\n",
        "        input = tf.concat((input_ids,mask),axis=-1)\n",
        "        return input\n",
        "        \n",
        "    def get_input_ids_and_word_ids(self, sentence):\n",
        "        tokenizer = self.get_tokenizer()\n",
        "        tokens = tokenizer.texts_to_sequences(sentence.split(' '))\n",
        "        print(tokens)\n",
        "        input_ids = []\n",
        "        word_ids = []\n",
        "        \n",
        "        for index, token in enumerate(tokens):\n",
        "            for sub_token in token:\n",
        "                if sub_token:\n",
        "                    input_ids.append(sub_token)\n",
        "                    word_ids.append(index)\n",
        "        return input_ids, word_ids\n",
        "\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        with open(self.config.tokenizer_path,'rb') as f:\n",
        "            tokenizer = pickle.load(f) \n",
        "        return tokenizer\n",
        "    \n",
        "    def get_model(self):\n",
        "        model = tf.keras.models.load_model(self.model_path, custom_objects={\n",
        "            'NERModel':NERModel,\n",
        "            'MaskedLoss':MaskedLoss,\n",
        "            'masked_acc': masked_acc,\n",
        "            'CustomSchedule': CustomSchedule\n",
        "        })\n",
        "        return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-04-29 02:17:26,591: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
            "[2024-04-29 02:17:26,594: INFO: common: yaml file: params.yaml loaded successfully]\n",
            "[2024-04-29 02:17:26,594: INFO: common: created directory at: artifacts]\n",
            "[2024-04-29 02:17:26,597: INFO: common: created directory at: artifacts/data_transformation]\n",
            "[2024-04-29 02:17:26,598: INFO: common: created directory at: artifacts/model_evaluation]\n",
            "[[636], [1739], [1], [993], [9], [1], [1], [392], [1], [4], [2], [457], [1], [639], [31], [285], [31], [49], [502], [1], [6], [987], [22], [5788], [3], [2], [1301], [94], [7269]]\n",
            "1/1 [==============================] - 0s 238ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[['Ukraine', 'Location'], ['CNN.', 'Miscellaneous']]"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PredictionPipeline().predict(\"Ukraine launched drone attacks on Russia's Kushchevsk military airfield in the southern Krasnodar region, as well as two oil refineries, a source with knowledge of the operation told CNN.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
